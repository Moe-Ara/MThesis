import argparse
import csv
import json
from pathlib import Path
from typing import Dict, Iterable, List, Sequence, Tuple


def load_dataset(path: Path, key_field: str = "id") -> Dict[str, Dict[str, str]]:
    """Load the CSV dataset keyed by the provided unique field."""
    with path.open("r", encoding="utf-8", newline="") as csvfile:
        reader = csv.DictReader(csvfile)
        return {row[key_field]: row for row in reader}


def format_log_fields(row: Dict[str, str], field_order: Sequence[str]) -> str:
    """Serialize the selected fields into a human-readable block for the prompt."""
    lines: List[str] = []
    for field in field_order:
        value = row.get(field, "")
        if value:
            lines.append(f"{field}: {value}")
    return "\n".join(lines)


def build_prompt(
    log_block: str,
    detection_goal: str,
    existing_description: str | None = None,
) -> str:
    """Construct a consistent prompt for fine-tuning."""
    parts: List[str] = [
        "You are an expert Wazuh detection engineer.",
        f"Detection goal: {detection_goal}",
        "Sample log event:",
        log_block,
    ]
    if existing_description:
        parts.append(f"Current description: {existing_description}")
    parts.append(
        "Produce a precise Wazuh custom rule (XML) that would detect this behavior."
    )
    return "\n\n".join(parts)


def build_completion(completion_text: str) -> str:
    """Wrap the desired completion so the model learns to output XML."""
    return completion_text.strip()


def write_jsonl(
    annotations: Iterable[Tuple[str, str, str]],
    output_path: Path,
) -> None:
    """Dump a simple prompt/completion JSONL file."""
    with output_path.open("w", encoding="utf-8", newline="") as out_file:
        for prompt, completion, example_id in annotations:
            obj = {"prompt": prompt, "completion": completion}
            out_file.write(json.dumps(obj, ensure_ascii=False) + "\n")


def parse_annotations(path: Path) -> List[Dict[str, str]]:
    with path.open("r", encoding="utf-8") as f:
        data = json.load(f)
    if not isinstance(data, list):
        raise ValueError("Annotations file must contain a list of entries.")
    return data


def main() -> None:
    parser = argparse.ArgumentParser(
        description="Produce prompt/completion pairs for Ollama training."
    )
    parser.add_argument(
        "--dataset",
        type=Path,
        required=True,
        help="CSV dataset generated by dataset_builder.py.",
    )
    parser.add_argument(
        "--annotations",
        type=Path,
        required=True,
        help="JSON list describing goals and expected rule XML for specific rows.",
    )
    parser.add_argument(
        "--output",
        type=Path,
        required=True,
        help="Destination JSONL file for Ollama fine-tuning pairs.",
    )
    parser.add_argument(
        "--dataset-key",
        default="id",
        help="Field in the CSV used to lookup rows in annotations (default: id).",
    )
    parser.add_argument(
        "--fields",
        nargs="+",
        default=["timestamp", "rule", "agent", "location", "full_log"],
        help="Fields from the CSV to include in the prompt (default: timestamp rule agent location full_log).",
    )

    args = parser.parse_args()
    dataset = load_dataset(args.dataset, key_field=args.dataset_key)
    annotations = parse_annotations(args.annotations)

    training_pairs: List[Tuple[str, str, str]] = []
    for entry in annotations:
        dataset_id = entry.get("dataset_id")
        if not dataset_id:
            raise ValueError("Each annotation entry must include a dataset_id.")
        row = dataset.get(dataset_id)
        if not row:
            raise KeyError(f"No row found for dataset_id {dataset_id}.")

        log_block = format_log_fields(row, args.fields)
        detection_goal = entry.get("detection_goal", "Detect suspicious activity.")
        prompt = build_prompt(
            log_block, detection_goal, existing_description=row.get("rule")
        )
        completion_text = entry.get("completion", entry.get("rule_xml"))
        if not completion_text:
            raise ValueError("Each annotation entry must provide 'completion' or 'rule_xml'.")

        completion = build_completion(completion_text)
        training_pairs.append((prompt, completion, dataset_id))

    args.output.parent.mkdir(parents=True, exist_ok=True)
    write_jsonl(training_pairs, args.output)
    print(
        json.dumps(
            {
                "pairs_written": len(training_pairs),
                "dataset": str(args.dataset),
                "annotations": str(args.annotations),
                "output": str(args.output),
            },
            indent=2,
        )
    )


if __name__ == "__main__":
    main()
